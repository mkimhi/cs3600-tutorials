{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    ".\n",
    "\n",
    "<img src=\"../3600.jpg\" width=500 />\n",
    "\n",
    "# Tutorial 4: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Todays Agenda\n",
    "---\n",
    "\n",
    "- What RNNs are and how they work\n",
    "- Embedding\n",
    "- impplementation of basic RNN\n",
    "- Sentiment analysis for movie reviews\n",
    "- reading at home for LSTM and GRU, B-directional RNN and some NLP basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminders and recaps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Last tutorial we learned about CNNs- Conv layers to perform feature extranction, and FC layers for classification.\n",
    "\n",
    "- the convolution layers output does is not relay on the filter size, but on the input size. \n",
    "- the FC layers depend only on the input and output size.\n",
    "\n",
    "<center><img src=\"resources/A-vanilla-Convolutional-Neural-Network-CNN-representation.png\" width=\"600\" /></center>\n",
    "\n",
    "[image from](https://www.researchgate.net/figure/A-vanilla-Convolutional-Neural-Network-CNN-representation_fig2_339447623)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, we only talked about stationary data, not about **persistent state**. \n",
    "\n",
    "what do we do when there is another dimention? \n",
    "\n",
    "<center><img src=\"resources/vid.gif\" width=\"600\" /></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to look at previus inputs and outputs\n",
    "\n",
    "<center><img src=\"resources/crossing_tracking_Demo1.gif\" width=\"600\" /></center>\n",
    "\n",
    "<center><img src=\"resources/rnn.gif\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The idea is to produce output, like simple FC layer, but also produce a state, that we will denote by $\\vec{h}_t$ (hidden state)<br>\n",
    "An RNN has two inputs:\n",
    "\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both inputs:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{R}^{d_{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "as such\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases.\n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used.\n",
    "\n",
    "\n",
    ":\n",
    "<img src=\"resources/rnn_cell.png\" width=\"300\"/>\n",
    "<img src=\"resources/rnn_g.gif\" width=\"300\"/>\n",
    "\n",
    "\n",
    "\n",
    "note that:\n",
    "- The layer does not dependent on time\n",
    "- The same layer is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<center><img src=\"resources/rnn_unrolled.png\" width=\"1200\" /></center>\n",
    "\n",
    "\n",
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<center><img src=\"resources/rnn_layered.png\" width=\"1200\"/></center>\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN models are very flexible in terms of input and output meaning.\n",
    "\n",
    "### Forms of tasks we can preform now:\n",
    "\n",
    "* One-to-one: from fixed-sized input to fixed-sized output (e.g. image classification).\n",
    "* One-to-many: Sequence output (e.g. image captioning takes an image and outputs a sentence of words).\n",
    "* Many-to-one: Sequence input (e.g. sentiment analysis)\n",
    "* Many-to-many: Sequence input and sequence output (e.g. Machine Translation).\n",
    "* Many-to-many: Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).\n",
    "\n",
    "<center><img src=\"resources/rnn_use_cases.jpeg\" width=\"1200\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Forward pass seems pretty easy...<br>\n",
    "What about **backpropagation** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop from Loss for RNN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"resources/bptt.png\" width=\"1000\"></center>\n",
    "\n",
    "**Backpropagation through time**, or BPTT\n",
    "\n",
    "1. Loss is a sum of losses from all outputs\n",
    "\n",
    "$$ L =\\sum_{k=1}^{t}L_k $$\n",
    "\n",
    "2. Calculate Gradient of each loss w.r.t. each parameter at each timestep\n",
    "\n",
    "$$\n",
    "\\pderiv{L_t}{\\mat{W}} = \\sum_{k=1}^{t}\n",
    "\\pderiv{L_t}{\\hat y_t} \\cdot\n",
    "\\pderiv{\\hat y_t}{\\vec{h}_t} \\cdot\n",
    "\\pderiv{\\vec{h}_t}{\\vec{h}_k} \\cdot\n",
    "\\pderiv{\\vec{h}_k}{\\mat{W}}\n",
    "$$\n",
    "\n",
    "3. For each parameter, accumulate gradients from all timesteps\n",
    "\n",
    "$$\n",
    "\\pderiv{L}{\\mat{W_i}} = \\sum_{k=1}^{t}\n",
    "\\pderiv{L_t}{\\hat {W_i}_t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see, the chain rule here can create a really long chain...\n",
    "Remember the Vanishing gradients problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "one naive way to limit the vanishing gradients problem is to trancate the gradients with a fixed lenght.\n",
    "**Truncated backpropagation through time** or **TBPTT**\n",
    "\n",
    "<center><img src=\"resources/tbptt.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the above equations, let's create a simple RNN layer  with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=10, bias=False)\n",
       "  (fc_hh): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc_hy): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 10, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And manually \"run\" a few time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1 ((3, 1)):\n",
      "tensor([[0.4819],\n",
      "        [0.4028],\n",
      "        [0.5856]], grad_fn=<SigmoidBackward>)\n",
      "h1 ((3, 10)):\n",
      "tensor([[-0.2590, -0.1616, -0.2057, -0.3401,  0.3149,  0.2235,  0.2404,  0.5265,\n",
      "          0.3989,  0.1572],\n",
      "        [-0.3304,  0.3464, -0.5787,  0.7095, -0.0218,  0.0671, -0.2577,  0.7325,\n",
      "          0.1604, -0.3398],\n",
      "        [-0.2100, -0.4477,  0.6644, -0.4720,  0.0551,  0.6401, -0.2490,  0.5469,\n",
      "         -0.3066, -0.2985]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y2 ((3, 1)):\n",
      "tensor([[0.5906],\n",
      "        [0.5013],\n",
      "        [0.5614]], grad_fn=<SigmoidBackward>)\n",
      "h2 ((3, 10)):\n",
      "tensor([[ 0.9190, -0.4307, -0.5411, -0.3046,  0.2485,  0.8902,  0.3464,  0.5446,\n",
      "          0.7853, -0.4272],\n",
      "        [ 0.3684, -0.7912, -0.2990,  0.4389,  0.8003,  0.8202,  0.8060,  0.2826,\n",
      "          0.5571, -0.4344],\n",
      "        [ 0.7946, -0.2560,  0.6325, -0.3587,  0.4913,  0.0471, -0.0600,  0.6511,\n",
      "         -0.7053, -0.1652]], grad_fn=<TanhBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# t=1\n",
    "x1 = torch.randn(N, in_dim, requires_grad=True) # requiring grad just for torchviz\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1 ({tuple(y1.shape)}):\\n{y1}')\n",
    "print(f'h1 ({tuple(h1.shape)}):\\n{h1}\\n')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim, requires_grad=True)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2 ({tuple(y2.shape)}):\\n{y2}')\n",
    "print(f'h2 ({tuple(h2.shape)}):\\n{h2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, let's visualize the computation graph and see what happened when we used the same RNN block twice, by looking at the graph from both $y_1$ and $y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"653pt\" height=\"688pt\"\n",
       " viewBox=\"0.00 0.00 653.00 688.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 684)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-684 649,-684 649,4 -4,4\"/>\n",
       "<!-- 140534538878208 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140534538878208</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"494,-31 435,-31 435,0 494,0 494,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"464.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (3, 1)</text>\n",
       "</g>\n",
       "<!-- 140534396680272 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140534396680272</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"518,-86 411,-86 411,-67 518,-67 518,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"464.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward</text>\n",
       "</g>\n",
       "<!-- 140534396680272&#45;&gt;140534538878208 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>140534396680272&#45;&gt;140534538878208</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464.5,-66.79C464.5,-60.07 464.5,-50.4 464.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"468,-41.19 464.5,-31.19 461,-41.19 468,-41.19\"/>\n",
       "</g>\n",
       "<!-- 140534396681184 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140534396681184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"512,-141 417,-141 417,-122 512,-122 512,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"464.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140534396681184&#45;&gt;140534396680272 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140534396681184&#45;&gt;140534396680272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464.5,-121.75C464.5,-114.8 464.5,-104.85 464.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"468,-96.09 464.5,-86.09 461,-96.09 468,-96.09\"/>\n",
       "</g>\n",
       "<!-- 140534396680608 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140534396680608</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"402,-196 301,-196 301,-177 402,-177 402,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396680608&#45;&gt;140534396681184 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140534396680608&#45;&gt;140534396681184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M369.66,-176.98C387.75,-168.5 415.78,-155.35 436.69,-145.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"438.45,-148.59 446.01,-141.17 435.47,-142.25 438.45,-148.59\"/>\n",
       "</g>\n",
       "<!-- 140534396763776 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140534396763776</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"390,-262 313,-262 313,-232 390,-232 390,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">fc_hy.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"351.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 140534396763776&#45;&gt;140534396680608 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140534396763776&#45;&gt;140534396680608</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.5,-231.84C351.5,-224.21 351.5,-214.7 351.5,-206.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"355,-206.27 351.5,-196.27 348,-206.27 355,-206.27\"/>\n",
       "</g>\n",
       "<!-- 140534396679792 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140534396679792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"509,-196 420,-196 420,-177 509,-177 509,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"464.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 140534396679792&#45;&gt;140534396681184 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140534396679792&#45;&gt;140534396681184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464.5,-176.75C464.5,-169.8 464.5,-159.85 464.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"468,-151.09 464.5,-141.09 461,-151.09 468,-151.09\"/>\n",
       "</g>\n",
       "<!-- 140534396679840 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140534396679840</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"503,-256.5 414,-256.5 414,-237.5 503,-237.5 503,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"458.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140534396679840&#45;&gt;140534396679792 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140534396679840&#45;&gt;140534396679792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M459.39,-237.37C460.22,-229.25 461.49,-216.81 462.56,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"466.07,-206.47 463.61,-196.17 459.11,-205.76 466.07,-206.47\"/>\n",
       "</g>\n",
       "<!-- 140534396681280 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140534396681280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"193,-493 116,-493 116,-474 193,-474 193,-493\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-481\" font-family=\"monospace\" font-size=\"10.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 140534396681280&#45;&gt;140534396679840 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140534396681280&#45;&gt;140534396679840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165.68,-473.88C211.02,-438.9 381.52,-307.38 439.25,-262.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"441.44,-265.58 447.22,-256.7 437.16,-260.04 441.44,-265.58\"/>\n",
       "</g>\n",
       "<!-- 140534396679744 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140534396679744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-548 0,-548 0,-529 101,-529 101,-548\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-536\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396679744&#45;&gt;140534396681280 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140534396679744&#45;&gt;140534396681280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.21,-528.98C83.71,-520.57 109.2,-507.59 128.38,-497.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"130.17,-500.83 137.49,-493.17 126.99,-494.59 130.17,-500.83\"/>\n",
       "</g>\n",
       "<!-- 140534538878144 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140534538878144</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"89,-614 12,-614 12,-584 89,-584 89,-614\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-602\" font-family=\"monospace\" font-size=\"10.00\">x2</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\"> (3, 1024)</text>\n",
       "</g>\n",
       "<!-- 140534538878144&#45;&gt;140534396679744 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140534538878144&#45;&gt;140534396679744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-583.84C50.5,-576.21 50.5,-566.7 50.5,-558.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-558.27 50.5,-548.27 47,-558.27 54,-558.27\"/>\n",
       "</g>\n",
       "<!-- 140534396681424 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140534396681424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"190,-548 119,-548 119,-529 190,-529 190,-548\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5\" y=\"-536\" font-family=\"monospace\" font-size=\"10.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140534396681424&#45;&gt;140534396681280 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140534396681424&#45;&gt;140534396681280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.5,-528.75C154.5,-521.8 154.5,-511.85 154.5,-503.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158,-503.09 154.5,-493.09 151,-503.09 158,-503.09\"/>\n",
       "</g>\n",
       "<!-- 140534396679552 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140534396679552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"250,-608.5 149,-608.5 149,-589.5 250,-589.5 250,-608.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-596.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396679552&#45;&gt;140534396681424 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140534396679552&#45;&gt;140534396681424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.86,-589.37C186.21,-580.73 175.81,-567.2 167.51,-556.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.04,-553.96 161.17,-548.17 164.49,-558.23 170.04,-553.96\"/>\n",
       "</g>\n",
       "<!-- 140534396679504 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>140534396679504</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"279,-548 208,-548 208,-529 279,-529 279,-548\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.5\" y=\"-536\" font-family=\"monospace\" font-size=\"10.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140534396679552&#45;&gt;140534396679504 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>140534396679552&#45;&gt;140534396679504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M205.99,-589.37C212.49,-580.73 222.66,-567.2 230.78,-556.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"233.77,-558.26 236.98,-548.17 228.17,-554.06 233.77,-558.26\"/>\n",
       "</g>\n",
       "<!-- 140534486786240 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140534486786240</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"244,-680 155,-680 155,-650 244,-650 244,-680\"/>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-668\" font-family=\"monospace\" font-size=\"10.00\">fc_xh.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"199.5\" y=\"-657\" font-family=\"monospace\" font-size=\"10.00\"> (10, 1024)</text>\n",
       "</g>\n",
       "<!-- 140534486786240&#45;&gt;140534396679552 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140534486786240&#45;&gt;140534396679552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M199.5,-649.8C199.5,-640.7 199.5,-628.79 199.5,-618.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203,-618.84 199.5,-608.84 196,-618.84 203,-618.84\"/>\n",
       "</g>\n",
       "<!-- 140534396680464 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>140534396680464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"508,-322.5 413,-322.5 413,-303.5 508,-303.5 508,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140534396680464&#45;&gt;140534396679840 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140534396680464&#45;&gt;140534396679840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M460.23,-303.37C459.94,-294.16 459.48,-279.29 459.1,-267.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"462.59,-266.79 458.78,-256.91 455.59,-267.01 462.59,-266.79\"/>\n",
       "</g>\n",
       "<!-- 140534396678832 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>140534396678832</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"645,-548 544,-548 544,-529 645,-529 645,-548\"/>\n",
       "<text text-anchor=\"middle\" x=\"594.5\" y=\"-536\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396678832&#45;&gt;140534396680464 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>140534396678832&#45;&gt;140534396680464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M593.37,-528.92C590.4,-508.63 581.09,-456.57 559.5,-419 538.25,-382.02 501.48,-347.79 479.23,-329\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"481.41,-326.26 471.47,-322.59 476.95,-331.66 481.41,-326.26\"/>\n",
       "</g>\n",
       "<!-- 140534396679360 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>140534396679360</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"487,-493 392,-493 392,-474 487,-474 487,-493\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-481\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140534396678832&#45;&gt;140534396679360 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>140534396678832&#45;&gt;140534396679360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M569.6,-528.98C543.8,-520.16 503.28,-506.31 474.32,-496.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"475.19,-493.01 464.6,-493.08 472.93,-499.63 475.19,-493.01\"/>\n",
       "</g>\n",
       "<!-- 140534396050880 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>140534396050880</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"633,-614 556,-614 556,-584 633,-584 633,-614\"/>\n",
       "<text text-anchor=\"middle\" x=\"594.5\" y=\"-602\" font-family=\"monospace\" font-size=\"10.00\">fc_hh.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"594.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 140534396050880&#45;&gt;140534396678832 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>140534396050880&#45;&gt;140534396678832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M594.5,-583.84C594.5,-576.21 594.5,-566.7 594.5,-558.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"598,-558.27 594.5,-548.27 591,-558.27 598,-558.27\"/>\n",
       "</g>\n",
       "<!-- 140534396680416 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>140534396680416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"469,-383 380,-383 380,-364 469,-364 469,-383\"/>\n",
       "<text text-anchor=\"middle\" x=\"424.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward</text>\n",
       "</g>\n",
       "<!-- 140534396680416&#45;&gt;140534396680464 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>140534396680416&#45;&gt;140534396680464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M429.81,-363.87C435.08,-355.31 443.29,-341.97 449.89,-331.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"452.91,-333.02 455.17,-322.67 446.94,-329.35 452.91,-333.02\"/>\n",
       "</g>\n",
       "<!-- 140534396681808 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>140534396681808</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"462,-438 373,-438 373,-419 462,-419 462,-438\"/>\n",
       "<text text-anchor=\"middle\" x=\"417.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 140534396681808&#45;&gt;140534396680416 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>140534396681808&#45;&gt;140534396680416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M418.66,-418.75C419.57,-411.8 420.89,-401.85 422.04,-393.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"425.53,-393.46 423.37,-383.09 418.59,-392.54 425.53,-393.46\"/>\n",
       "</g>\n",
       "<!-- 140534396680176 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>140534396680176</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"374,-493 297,-493 297,-474 374,-474 374,-493\"/>\n",
       "<text text-anchor=\"middle\" x=\"335.5\" y=\"-481\" font-family=\"monospace\" font-size=\"10.00\">MmBackward</text>\n",
       "</g>\n",
       "<!-- 140534396680176&#45;&gt;140534396681808 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>140534396680176&#45;&gt;140534396681808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.67,-473.98C361.27,-465.84 380.5,-453.41 395.44,-443.76\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"397.59,-446.54 404.09,-438.17 393.79,-440.66 397.59,-446.54\"/>\n",
       "</g>\n",
       "<!-- 140534396679072 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>140534396679072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"398,-548 297,-548 297,-529 398,-529 398,-548\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-536\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396679072&#45;&gt;140534396680176 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>140534396679072&#45;&gt;140534396680176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M345.52,-528.75C343.93,-521.72 341.64,-511.62 339.65,-502.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"343.07,-502.07 337.44,-493.09 336.24,-503.62 343.07,-502.07\"/>\n",
       "</g>\n",
       "<!-- 140534395689216 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>140534395689216</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"386,-614 309,-614 309,-584 386,-584 386,-614\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-602\" font-family=\"monospace\" font-size=\"10.00\">x1</text>\n",
       "<text text-anchor=\"middle\" x=\"347.5\" y=\"-591\" font-family=\"monospace\" font-size=\"10.00\"> (3, 1024)</text>\n",
       "</g>\n",
       "<!-- 140534395689216&#45;&gt;140534396679072 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>140534395689216&#45;&gt;140534396679072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M347.5,-583.84C347.5,-576.21 347.5,-566.7 347.5,-558.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"351,-558.27 347.5,-548.27 344,-558.27 351,-558.27\"/>\n",
       "</g>\n",
       "<!-- 140534396679504&#45;&gt;140534396680176 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>140534396679504&#45;&gt;140534396680176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M258.28,-528.98C272.68,-520.69 294.81,-507.94 311.69,-498.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"313.53,-501.19 320.45,-493.17 310.04,-495.13 313.53,-501.19\"/>\n",
       "</g>\n",
       "<!-- 140534396679360&#45;&gt;140534396681808 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>140534396679360&#45;&gt;140534396681808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M435.87,-473.75C432.92,-466.65 428.67,-456.4 424.99,-447.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"428.13,-445.98 421.07,-438.09 421.67,-448.67 428.13,-445.98\"/>\n",
       "</g>\n",
       "<!-- 140534396678496 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>140534396678496</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"487,-548 416,-548 416,-529 487,-529 487,-548\"/>\n",
       "<text text-anchor=\"middle\" x=\"451.5\" y=\"-536\" font-family=\"monospace\" font-size=\"10.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140534396678496&#45;&gt;140534396679360 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>140534396678496&#45;&gt;140534396679360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M449.52,-528.75C447.93,-521.72 445.64,-511.62 443.65,-502.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"447.07,-502.07 441.44,-493.09 440.24,-503.62 447.07,-502.07\"/>\n",
       "</g>\n",
       "<!-- 140534396682192 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>140534396682192</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"522,-608.5 421,-608.5 421,-589.5 522,-589.5 522,-608.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"471.5\" y=\"-596.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396682192&#45;&gt;140534396678496 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>140534396682192&#45;&gt;140534396678496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M468.55,-589.37C465.71,-581.07 461.34,-568.28 457.73,-557.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"461.01,-556.5 454.46,-548.17 454.39,-558.76 461.01,-556.5\"/>\n",
       "</g>\n",
       "<!-- 140534396680992 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>140534396680992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"551,-438 480,-438 480,-419 551,-419 551,-438\"/>\n",
       "<text text-anchor=\"middle\" x=\"515.5\" y=\"-426\" font-family=\"monospace\" font-size=\"10.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140534396682192&#45;&gt;140534396680992 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>140534396682192&#45;&gt;140534396680992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M476.52,-589.45C482.01,-579.74 490.66,-563.23 495.5,-548 506.4,-513.65 511.7,-471.7 513.99,-448.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"517.48,-448.3 514.88,-438.02 510.51,-447.67 517.48,-448.3\"/>\n",
       "</g>\n",
       "<!-- 140534395861184 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>140534395861184</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"516,-680 427,-680 427,-650 516,-650 516,-680\"/>\n",
       "<text text-anchor=\"middle\" x=\"471.5\" y=\"-668\" font-family=\"monospace\" font-size=\"10.00\">fc_hh.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"471.5\" y=\"-657\" font-family=\"monospace\" font-size=\"10.00\"> (10, 10)</text>\n",
       "</g>\n",
       "<!-- 140534395861184&#45;&gt;140534396682192 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>140534395861184&#45;&gt;140534396682192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M471.5,-649.8C471.5,-640.7 471.5,-628.79 471.5,-618.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"475,-618.84 471.5,-608.84 468,-618.84 475,-618.84\"/>\n",
       "</g>\n",
       "<!-- 140534396680992&#45;&gt;140534396680464 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>140534396680992&#45;&gt;140534396680464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M511.37,-418.97C502.41,-400.48 481.05,-356.4 469.05,-331.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"472.19,-330.1 464.68,-322.63 465.89,-333.16 472.19,-330.1\"/>\n",
       "</g>\n",
       "<!-- 140534396679408 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>140534396679408</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"602,-196 531,-196 531,-177 602,-177 602,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"566.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140534396679408&#45;&gt;140534396681184 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>140534396679408&#45;&gt;140534396681184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M550.11,-176.98C533.93,-168.57 508.93,-155.59 490.12,-145.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"491.67,-142.68 481.19,-141.17 488.45,-148.89 491.67,-142.68\"/>\n",
       "</g>\n",
       "<!-- 140534396681760 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>140534396681760</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"622,-256.5 521,-256.5 521,-237.5 622,-237.5 622,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"571.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 140534396681760&#45;&gt;140534396679408 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>140534396681760&#45;&gt;140534396679408</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M570.76,-237.37C570.07,-229.25 569,-216.81 568.11,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"571.58,-205.83 567.24,-196.17 564.61,-206.43 571.58,-205.83\"/>\n",
       "</g>\n",
       "<!-- 140534396049216 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>140534396049216</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"616,-328 527,-328 527,-298 616,-298 616,-328\"/>\n",
       "<text text-anchor=\"middle\" x=\"571.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">fc_hy.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"571.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (1, 10)</text>\n",
       "</g>\n",
       "<!-- 140534396049216&#45;&gt;140534396681760 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>140534396049216&#45;&gt;140534396681760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M571.5,-297.8C571.5,-288.7 571.5,-276.79 571.5,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"575,-266.84 571.5,-256.84 568,-266.84 575,-266.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fd0b6cafd00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "\n",
    "torchviz.make_dot(\n",
    "    y2, # Note: Change here to y2 to see the fullly unrolled graph!\n",
    "    params=dict(list(rnn.named_parameters()) + [('x1', x1), ('x2', x2)])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "---\n",
    "\n",
    "What is a **word embedding**? How do we get one?\n",
    "\n",
    "Embeddings encode tokens as tensors in a way that maintain some **semantic** meaning for our task.\n",
    "\n",
    "<center><img src=\"resources/word_embeddings.png\" width=\"700\"/></center>\n",
    "\n",
    "brief about ways to embed:\n",
    "\n",
    "1. Embedding Layer: We train an embeding layer with our desired size, take a long time and require a lot of data, but learn the specific relations in our corpus\n",
    "\n",
    "2. Word2Vec: statistical method for efficiently learning a standalone word embedding from a text corpus. it's based on CBOW and C-SKIP-GRAM (basics explained in Further reading).\n",
    "\n",
    "3. GloVe (Global Vectors for Word Representation):extension to the word2vec method for efficiently learning word vectors. using matrix factorization techniques such as Latent Semantic Analysis (LSA). if Word2Vec use a window to determine the context, GloVe used the statistics of a word to accure in the hole text.\n",
    "\n",
    "Here we will demostrate basic Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 0, 1, 0, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8127, -0.1854,  1.9355,  0.4339, -0.0432,  0.4081, -0.1311, -0.0034],\n",
       "        [-1.1351, -0.4598, -1.3482, -0.9486,  0.2552, -1.1558, -0.1664, -1.3288],\n",
       "        [ 0.1489, -0.3607, -1.4430, -0.3179, -0.9790,  0.2895,  0.2675, -0.5336],\n",
       "        [-1.1351, -0.4598, -1.3482, -0.9486,  0.2552, -1.1558, -0.1664, -1.3288],\n",
       "        [ 0.1489, -0.3607, -1.4430, -0.3179, -0.9790,  0.2895,  0.2675, -0.5336],\n",
       "        [ 0.8127, -0.1854,  1.9355,  0.4339, -0.0432,  0.4081, -0.1311, -0.0034]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=8)\n",
    "\n",
    "token_idx = torch.randint(low=0, high=5, size=(6,))\n",
    "print(token_idx)\n",
    "embedding_layer(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentiment analysis for movie reviews\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The task: Given a review about a movie written by some user, decide whether it's **positive**, **negative** or **neutral**.\n",
    "\n",
    "<center><img src=\"resources/sentiment_analysis.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Classically this is considered a challenging task if approached based on keywords alone.\n",
    "\n",
    "Consider:\n",
    "\n",
    "     \"This movie was actually neither that funny, nor super witty.\"\n",
    "     \n",
    "To comprehend such a sentence, it's intuitive to see that some \"state\" must be kept when \"reading\" it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we only worked with images or tabular data.\n",
    "all the optimization process we lean on works with neumerical parameters, so what can we do to work with numbers again?<br>\n",
    "\n",
    "<br>\n",
    "we will use the simple folowing steps:\n",
    "* Load the dataset\n",
    "* Decide on tokens and tokenize our dataset (words/ chars)\n",
    "* Build a vocabulary from all existing tokens\n",
    "* Encode each text/sentence into sequences of numerical numerical\n",
    "\n",
    "\n",
    "For the sentiment analysis task, we will use `torchtext` to accelerate the preperation process.<br>\n",
    "We're goint to use IMDB dataset: this dataset contains movie reviews which are labeled as positive and negative.<br>\n",
    "If you want to load other datasets or load a custom dataset: https://torchtext.readthedocs.io/en/latest/datasets.html\n",
    "\n",
    "Denote some Special tokens:\n",
    "* \\<sos> - start of sentence\n",
    "* \\<pad> - use to pad sentences so we can load batch of sentences with diffrent lenghts.\n",
    "* \\<eos> - end of a sentence.\n",
    "* \\<unk> - for workd that doesn't come in the train set at all or if the model thinks that no word in the vocabulary is good as next word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check what is the encoding assigned for `else` and what word belong to the token `10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n",
      "Number of test     samples: 2210\n"
     ]
    }
   ],
   "source": [
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')\n",
    "print(f'Number of test     samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0111 [positive]:\n",
      "review: the film aims to be funny , uplifting and moving , sometimes all at once .\n",
      "\n",
      "sample 4321 [neutral]:\n",
      "review: the most anti - human big studio picture since 3000 miles to graceland .\n",
      "\n",
      "sample 7777 [negative]:\n",
      "review: an ugly , revolting movie .\n",
      "\n",
      "sample 0000 [positive]:\n",
      "review: the rock is destined to be the 21st century 's new ` ` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean - claud van damme or steven segal .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([111, 4321, 7777, 0]):\n",
    "    example = ds_train[i]\n",
    "    label = example.label\n",
    "    review = str.join(\" \", example.text)\n",
    "    print(f'sample {i:04d} [{label}]:\\nreview: {review}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = [ds_train[i].label for i in range(len(ds_train))]\n",
    "plt.hist(hist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 15482\n",
      "Number of tokens in training labels: 3\n"
     ]
    }
   ],
   "source": [
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "vocab = review_parser.vocab\n",
    "print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 tokens:\n",
      " ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'the', ',', 'a', 'and', 'of', 'to', '-', 'is', \"'s\", 'it', 'that', 'in', 'as', 'but', 'film']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'first 20 tokens:\\n', review_parser.vocab.itos[:20], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=film            index=19\n",
      "word=actor           index=492\n",
      "word=schwarzenegger  index=3404\n",
      "word=spielberg       index=715\n",
      "index=19             is the word =film\n",
      "index=492            is the word =actor\n",
      "index=3404           is the word =schwarzenegger\n",
      "index=715            is the word =spielberg\n"
     ]
    }
   ],
   "source": [
    "# Show that some words exist in the vocab\n",
    "for w in ['film', 'actor', 'schwarzenegger', 'spielberg']:\n",
    "    print(f'word={w:15s} index={review_parser.vocab.stoi[w]}')\n",
    "    \n",
    "for i in [19,492,3404,715]:\n",
    "    print(f'index={str(i):14} is the word ={review_parser.vocab.itos[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels vocab:\n",
      " {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'labels vocab:\\n', dict(label_parser.vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      " tensor([[    2,     2,     2,     2],\n",
      "        [   23,   140,     7,    17],\n",
      "        [   12,  1011,   154,    14],\n",
      "        [ 2084,    61,  2530,   411],\n",
      "        [  396,    28,    41,    55],\n",
      "        [    6,  2780,   350,     6],\n",
      "        [   10, 10436,     6,    26],\n",
      "        [   29,    10,    77,    71],\n",
      "        [  400,   140,     6,   211],\n",
      "        [    6,    63,   880,   358],\n",
      "        [   18,     6,     6,   263],\n",
      "        [   14,    66,     8,     4],\n",
      "        [   13,   292,  1911,     3],\n",
      "        [ 1060,   114,     4,     1],\n",
      "        [   97,  3570,     3,     1],\n",
      "        [   41,     5,     1,     1],\n",
      "        [    8,   325,     1,     1],\n",
      "        [ 7271,    46,     1,     1],\n",
      "        [   20,   620,     1,     1],\n",
      "        [ 1973,    21,     1,     1],\n",
      "        [  388,  4526,     1,     1],\n",
      "        [    4,     4,     1,     1],\n",
      "        [    3,     3,     1,     1]]) torch.Size([23, 4])\n",
      "\n",
      "y = \n",
      " tensor([0, 1, 0, 0]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator creates batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)\n",
    "\n",
    "\n",
    "batch = next(iter(dl_train))\n",
    "\n",
    "X, y = batch.text, batch.label\n",
    "print('X = \\n', X, X.shape, end='\\n\\n')\n",
    "print('y = \\n', y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our sample tensor `X` is of shape `(sentence_length, batch_size)`, in our case batch is 1.\n",
    "\n",
    "Note that:\n",
    "1. `sentence_length` is fixed since we picked a max_len, otherwise it will change every batch! You can re-run the previous blocks without max_len.\n",
    "2. `sentence_length` dimension first,soon you will see why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "We'll now create our sentiment analysis model based on the simple `RNNLayer` we've implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=8)\n",
    "\n",
    "token_idx = torch.randint(low=0, high=5, size=(6,))\n",
    "print(token_idx)\n",
    "embedding_layer(token_idx)The model will:\n",
    "- Take an input batch of tokenized sentences.\n",
    "- Compute a dense word-embedding of each token.\n",
    "- Process the sentence **sequentially** through the RNN layer.\n",
    "- Produce a `(B, 3)` tensor, which we'll interpret as class probabilities for each sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, model time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_dim, embedding_dim)\n",
    "\n",
    "        # Our own Vanilla RNN layer, without phi_y so it outputs a class score\n",
    "        self.rnn = RNNLayer(in_dim=embedding_dim, h_dim=h_dim, out_dim=out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B) Note batch dim is not first!\n",
    "        \n",
    "        embedded = self.embedding(X) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Loop over (batch of) tokens in the sentence(s)\n",
    "        ht = None\n",
    "        for xt in embedded:           # xt is (B, E)\n",
    "            yt, ht = self.rnn(xt, ht) # yt is (B, D_out)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        \n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's instantiate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test a manual forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(X) = \n",
      " tensor([[-0.9353, -1.6935, -0.8588],\n",
      "        [-1.0673, -1.6069, -0.7863],\n",
      "        [-1.1683, -1.4031, -0.8136],\n",
      "        [-1.1687, -1.4023, -0.8137]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n",
      "labels =  tensor([0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'model(X) = \\n', model(X), model(X).shape)\n",
    "print(f'labels = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How big is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN model has 1,577,899 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The RNN model has {count_parameters(model):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why so many? We used only one RNN layer.\n",
    "\n",
    "Where are most of the weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Let's complete the example by showing the regular pytorch-style train loop with this model.\n",
    "\n",
    "We'll run only a few epochs on a small subset just to test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=100, max_batches=200):\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == max_batches-1:\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches):.3f}, accuracy={num_correct /(max_batches*BATCH_SIZE):.3f}, elapsed={time.time()-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.116, accuracy=0.388, elapsed=3.4 sec\n",
      "Epoch #1, loss=1.064, accuracy=0.396, elapsed=3.5 sec\n",
      "Epoch #2, loss=1.058, accuracy=0.407, elapsed=3.5 sec\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(rnn_model, optimizer, loss_fn, dl_train, max_epochs=3) # Demo, please don't use it for your HW as is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_accuracy(model, dataloader,show=True):\n",
    "    model.eval() # put in evaluation mode\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    confusion_matrix = np.zeros([3,3], int)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X, y = data.text.to(device), data.label.to(device)\n",
    "            y_pred_log_proba = model(X)\n",
    "            predicted = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            total += len(data)\n",
    "            total_correct += (predicted == y).sum().item()\n",
    "            for i, l in enumerate(y):\n",
    "                confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
    "\n",
    "    model_accuracy = total_correct / total * 100\n",
    "    print(\"Test accuracy: {:.3f}%\".format(model_accuracy))\n",
    "    if show:\n",
    "        labels = ('0','1','2')\n",
    "        fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "        ax.matshow(confusion_matrix, aspect='auto', vmin=0, vmax=1000, cmap=plt.get_cmap('Blues'))\n",
    "        plt.ylabel('Actual Category')\n",
    "        plt.yticks(range(3), labels)\n",
    "        plt.xlabel('Predicted Category')\n",
    "        plt.xticks(range(3), labels)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def print_stats(model, dataloader):\n",
    "    model.eval() # put in evaluation mode\n",
    "    trues = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            X, y = data.text.to(device), data.label.to(device)\n",
    "            trues+=list(y.cpu())\n",
    "            y_pred_log_proba = model(X)\n",
    "            predicted = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            preds+= list(predicted.cpu())            \n",
    "    print(metrics.classification_report(trues, preds, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 41.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAGFCAYAAADgn7rtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjd0lEQVR4nO3dd5gsdZ3v8fcHUBQkG1BQEVbQ1VVQVAQDYU17DaiYUATUxYCLenXV6xow3UXdu7iiIqwioqK7BmRNiIGjGNAFxCwGPIhI8IAEWZD0vX9UjQxNz0zXTM/0TPF+PU8/NV316+pvh3M+XVW/+lWqCkmS1B9rTboASZI0Xoa7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4b7zUCSLZMcleT3Sf6cZHWSdybZZNK1aeVJsleSw5KcnOSyJJXkI5OuSytTks2SPC/JcUl+leTKJJcm+WaS5yYxp+YhDmLTb0m2Ab4N3B44Hvg58EBgN+BMYJequmhyFWqlSXIGcF/gT8DvgHsAH62qZ02yLq1MSV4AHA6cB5wE/Ba4A/AkYCPgU8BTyrDqxHDvuSRfAh4JHFRVh02b/6/Ay4AjquoFk6pPK0+S3WhC/VfAw2n+QzbcNS9JdgfWBz5fVddPm7858D3gzsBeVfWpCZW4Irm7o8eSbE0T7KuB9wwsfgNwBbBPkvWXuDStYFV1UlX90i0pjUNVfa2qPjs92Nv55wPva+/uuuSFrXCGe7/t3k5PHPIP53LgW8B6wE5LXZgkjeCadnrtRKtYgQz3ftuunf5ihuW/bKfbLkEtkjSyJOsAz27vnjDJWlYiw73fNmqnl86wfGr+xotfiiR1cghwb+ALVfWlSRez0hjuN29ppx47lbRsJDkIeDnN2T37TLicFclw77epLfONZli+4UA7SZqoJAcC/wb8FNitqi6ecEkrkuHeb2e205mOqd+9nc50TF6SlkySlwLvBn5ME+znT7ailctw77eT2ukjB0d5SrIBsAtwJXDKUhcmSdMleRVwKHAGTbBfONmKVjbDvceq6tfAicBWwIEDi99IM3DEMVV1xRKXJkl/keR1NB3oTgP2qKo1Ey5pxXOEup4bMvzsz4AH0Qw/+wtgZ4efVRdJ9gT2bO9uDjwKOAs4uZ23pqpesfSVaSVKsi9wNHAdcBjD+wCtrqqjl7CsFc9wvxlIcmfgTcCjgc1oxnD+DPBGO6uoqyQH04xwOJOzq2qrpalGK90I3yeAr1fVrotfTX8Y7pIk9YzH3CVJ6hnDXZKknjHcJUnqGcNdkqSeMdwlSeoZw12SpJ4x3CVJ6hnD/WYmyQGTrkH94ndK4+Z3auEM95sf/9Fo3PxOadz8Ti2Q4S5JUs/0ZvjZrHPryi03mHQZy15deyVZ59aTLkM94ndqdDvc8y6TLmFF+MOaP3C7295u0mUse2efvZo1a9Zk2LJ1lrqYxZJbbsC62z110mVI0oy+9d13T7oE9cguD9pxxmXulpckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrGcJckqWcMd0mSesZwlySpZwx3SZJ6ZtmEe5ItkxyV5PdJ/pxkdZJ3Jtlk0rVJkrSSrDPpAgCSbAN8G7g9cDzwc+CBwEuARyfZpaoummCJkiStGMtly/29NMF+UFXtWVWvrqrdgUOB7YC3TrQ6SZJWkImHe5KtgUcCq4H3DCx+A3AFsE+S9Ze4NEmSVqSJhzuwezs9saqun76gqi4HvgWsB+y01IVJkrQSLYdw366d/mKG5b9sp9suQS2SJK14y6FD3Ubt9NIZlk/N33hwQZIDgAMAuMVtxl2XJEkr0nLYcp9L2mkNLqiqI6tqx6raMevceonLkiRpeVoO4T61Zb7RDMs3HGgnSZJmsRzC/cx2OtMx9bu305mOyUuSpGmWQ7if1E4fmeRG9STZANgFuBI4ZakLkyRpJZp4uFfVr4ETga2AAwcWvxFYHzimqq5Y4tIkSVqRlkNveYAX0Qw/+64kewA/Ax4E7EazO/6fJlibJEkrysS33OEvW+87AkfThPrLgW2AdwEPdlx5SZJGt1y23Kmqc4D9J12HJEkr3bLYcpckSeNjuEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPTNyuCfZYTELkSRJ49Fly/20JN9N8pwk6y1aRZIkaUG6hPsXgPsB/w78PslhSf5mccqSJEnzNXK4V9Vjga2ANwOXAQcCZyT5VpJnJ7nV4pQoSZK66NShrqrOraqDaUL+CcAXgQcCHwTOTXJoknuOu0hJkjS6efWWr6rrq+qz07bm3wRcDRwE/DjJqiR7ja9MSZI0qnGcCncv4D7AZkCAi4CHAv+R5LQkW43hOSRJ0ojmFe5Jbp/k1Ul+TbNrfk9gFfAkYHPgr4AjgO2B946jUEmSNJp1ujROsgfwfJrj7bcA/gi8Ezi8qn41relvgBclWRd46nhKlSRJoxg53JP8EtiaZtf7qTRb5B+vqqtmedgvgfUXVKEkSeqky5b7FsDRwHur6rQRH/NR4Dtdi5IkSfPXJdzvWFWXdll5VZ0DnNOtJEmStBBdOtRdnOTYRatEkiSNRZdwvxw4e7EKkSRJ49El3L8P/PViFSJJksajS7i/Dfi7JI9YrGIkSdLCdelQd3vgBOCLST4D/DdwPlCDDavqmLFUJ0mSOusS7kfTBHloRqJ7Ujt/erinvW+4S5I0IV3Cff9Fq0KSJI3NyOFeVR9azEIkSdJ4jOOqcJIkaRnpdOEYgCTr0Rxv3wHYGLgUOB04rqquGGt1kiSps65Xhfs74EPApjSd56YUcGiS/avqc2OsT5IkddTlqnD3Az4NrE1zQZivAecBdwR2B54BfDLJLh0uLCNJksasy5b7P9FsoT+0qk4ZWHZ0kvcAq4DXAE8eT3mSJKmrLh3qHgp8YkiwA1BV3wU+2baTJEkT0iXcN2Luy7f+Fthw/uVIkqSF6hLuvwceOEebHWmOw0uSpAnpEu5fAHZP8uoka09fkGStJC8H/rZtJ0mSJqRLh7o3A3sCbwWen+Rkmq30zYGHAFvRXEjmLeMtUZIkddFl+Nnzk+wCHAE8ArjrQJMvAy+oKnfLS5I0QZ0Gsamq1cCjkmxBM0LdRjQj1H2/qs4df3mSJKmrzsPPArRBbphLkrQMeeEYSZJ6psvws0eN0Ox64DLgZ8DnPP4uSdLS67Jbfj+a4WfhxheNmVID869J8tqqesc8a5MkSfPQZbf8NsDxwEXAa4FdgXu209e1848DHgQ8H7gAOCTJE8ZXriRJmkuXLfc9acaN336gZ/yZwDeSHAN8Hzi5qt6Z5ASa3fMvpvlRIEmSlkCXLfcDaC4cM7SXfFWdA3yibTd1/3PA/RZapCRJGl2XcN+K5pz22VwC3G3a/dXAbTpVJEmSFqRLuK+hGZluNo+kOfY+ZWPm/kEgSZLGqEu4fwq4X5KPJLnL9AVJ7pLko8D2NNd0n3J/4JcLrlKSJI2sS4e619N0qNsbeFqSc2l6xN8B2AJYGzijbUeSOwLXAB8eY72SJGkOXS4cc1mSnYFXAvsCWwNTW/BnAccAb6+qq9r25wE7j7dcSZI0l64XjvkzzaVf35xkA2BD4LKqunwxipMkSd3N68IxAG2gG+qSJC0zncM9ye2AJ9OMTrd+VT1v2vy7AT+qqivHWqUkSRpZp3BP8lzgXcCtaMaRL+B57eI7AN+hGcTmA2OsUZIkdTDyqXBJHgEcCfwCeCJw+PTlVfVj4Cc0w9RKkqQJ6bLl/irgPODhbc/5HYa0+SHw4LFUJkmS5qXLIDY70lyj/bJZ2vwO2HxhJUmSpIXoEu63BK6Yo83GwHXzrkaSJC1Yl3BfTTOc7GweRHMJWEmSNCFdwv144KFJnjJsYZL9gfvQjEEvSZImpEuHurcDTwc+lmQvYCOAJC+mGXP+STQXiTls3EVKkqTRdRlb/o9JHk4zhvz0rfd3tdOTgb2raq7j8pIkaRF1HVv+t8CuSe5Dc8rbZjTXaz+lqk5bhPokSVJH8xpbvqp+SHNOuyRJWma6jFB3VpKD5mhzYJKzFl6WJEmary695beiOY99NhsDd51nLZIkaQy6hPsobgNcPeZ1SpKkDmY95p7kLgOzNh4yD2Bt4C7AXoC75SVJmqC5OtStprms65SXtLeZBPjfC6xJkiQtwFzhfgxNuAd4Nk0P+TOGtLsOuAj4alWdOM4CJUlSN7OGe1XtN/V3kmcDx1XVmxa7KEmSNH9dRqgbd+c7SZK0CAxsSZJ6pvMIdUkeADwK2AJYd0iTqqrnLrQwSZI0PyOHe5IARwPPoulgN9XRbkpNm2+4S5I0IV12y78Y2Af4MLAjTZC/E9gZeA1wOfBxYOvxlihJkrroslt+X+DMqR70zYY8l1TVKcApSb4EnAJ8GfjgmOuUJEkj6rLlvh3wtYF5f/lxUFXfBz4HvGgMdUmSpHnqEu6huXb7lCuATQfa/BK4x0KLkiRJ89cl3M+l6SE/5Szg/gNt7k4T+iNLsleSw5KcnOSyJJXkI13WIUmSbtAl3L/HjcP8i8ADk7wuyb2SHAg8gea4exevpemstz3NDwhJkrQAXcL9U8DaSe7W3n87cDbwRpox5w8DLgFe3bGGlwHbAhsCL+z4WEmSNKDL8LOfAT4z7f7FSXYA/h7YhuYKcsdU1XldCqiqk6b+bnvgS5KkBeg8Qt10VXUp8C9jqkWSJI2BY8tLktQzs4Z7kg2TXJjk9CS3mKXdLZOcluT8JLcZf5kzPu8BSU5Ncmpde+VSPa0kScvaXFvu+wG3BQ6sqmtmalRVVwMHArcH9h9bdXOoqiOraseq2jHr3HqpnlaSpGVtrnB/HPDjqvrOXCtqh6H9AbDnGOqSJEnzNFe43wf4Zof1fQe49/zLkSRJCzVXuG8CXNRhfRcBG8+7GkmStGBzhfufaAJ+VJvQcfhZSZI0XnOd534WzfXaR7Vz+5iRJdmTG47Tb95OH5zk6PbvNVX1ii7rlCTp5myucP8K8I9JHl1VJ8zWMMkjacaHf1vHGranuVb8dFu3N2iGuDXcJUka0Vy75d8NXA18JMkeMzVKsjtwLHBV+5iRVdXBVZVZblt1WZ8kSTd3s265V9XvkvwDcCRwYpJTgK8CvwMK2BLYA3gwzfXen1dVXtlNkqQJmnNs+ap6f5L/obnq24OBnQaaBLgYOKiqjh1/iZIkqYuRLhxTVccm+S9gL+AhwB1pQv33NOfBf7Kq/rRoVUqSpJF1ueTrn4Cj25skSVqmvCqcJEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzMw5ik6TTpVunqaraZp6PlSRJCzTbCHVr0VwcpqvMsxZJkjQGM4a7l1qVJGll8pi7JEk9Y7hLktQzI18VbkqSdYEHAFsA6w5rU1XHLLAuSZI0T53CPclzgLcDm8zUhKYTnuEuSdKEjLxbPsmjgfcD5wGvoAny44F/Ar7c3v8E8JzxlylJkkbV5Zj7y4GLgJ2r6tB23hlVdUhVPRr4e+BJwK/HXKMkSeqgS7jfD/hsVV0+7PFV9QHgWzRb8pIkaUK6hPv6NLvkp1wFbDjQ5lTgQQstSpIkzV+XcD8fuN20++cB2w202QhYe6FFSZKk+esS7j/hxmF+MrBHkocCJLk38NS2nSRJmpAu4f5FYJckd2rvvx24DliV5A/AD4ANgLeMt0RJktRFl3A/gmbgmjUAVfVTYA+a0F8DnAg8pqq+MO4iJUnS6EYexKaqrgEuGJh3CvDYcRclSZLmz7HlJUnqGcNdkqSeGXm3fJLracaNn0tVVecL0kiSpPHoEsLfYHi4bwxsC9yapsf8JQuuSpIkzVuXDnW7zrQsyQbAocDONOPLS5KkCRnLMfd2vPkDgGuBt45jnZIkaX7G1qGuqq4HTgL2HNc6JUlSd+PuLX8rYJMxr1OSJHUwtnBPcg/gKcCvxrVOSZLUXZdT4Y6aZR13BnahuSLcy8dQlyRJmqcup8LtN8fynwPvqKoPzr8cSZK0UF3C/W4zzL8e+GNV/WkM9UiSpAXqcp772YtZiCRJGo+RO9QlOSrJ4+do89hZjs1LkqQl0KW3/H7A9nO0uS+w73yLkSRJCzfu89zXBa4b8zolSVIHXcN9xqvCJVkXeBhw/oIqkiRJCzJrh7okZw3MelmS/Yc0XRu4Hc2W+/vGVFsn62+2Kds/6+mTeGr11Kn/cdykS5CkeZmrt/xa3LC1XkDa26BrgB8BXwXeMrbqJElSZ7OGe1VtNfV3kuuBQ6vqTYtdlCRJmr8ug9jsBqxepDokSdKYdBnE5uuLWYgkSRqPLoPYvDbJNUm2mGH5nZJcneTV4ytPkiR11eVUuMcBq6rq3GELq+r3wEnAE8ZRmCRJmp8u4f5XwE/naPPTtp0kSZqQLuG+HvA/c7S5Cthg/uVIkqSF6hLu5wA7zdFmJ2DobntJkrQ0uoT7CcDDkjxt2MIkTwceDnxxHIVJkqT56XKe+9uAZwLHtgF/As1W+hbAY4DHAxcDh4y7SEmSNLou57mfm+RRwCeAPblxr/jQDHDzlKr63TgLlCRJ3XTZcqeqTk2yLc1pcTsBGwOXAKcAn62qa8ZdoCRJ6qZTuAO0Af7p9nYjSdYCHldVx4+hNkmSNA+dw32YJHcFngfsD9yR5hKwkiRpAuYd7knWpjnufgDwt9xwedivjKc0SZI0H53DPcnWNFvp+wF3aGevAY4APlBVZ4+tOkmS1NlI4Z5kHeCJNFvpu9FspV9Nc9z9ycDxVfX6xSpSkiSNbtZwT3J34O+BfYHb0pzydjpwNHBsVV2c5PrFLlKSJI1uri33M2mOo18IHAp8sKp+suhVSZKkeRtl+NkCvgB80mCXJGn5myvcXwecTXOK27eS/DTJK5PccfFLkyRJ8zFruFfVW6tqG5qx448DtqEZO/63ST6f5KlLUKMkSepgpKvCVdWXqmov4M7Aa2i25h8DfIxmt/32Se6/aFVKkqSRdbnkK1V1YVUdUlV/BTwC+CRwDbAj8L0k309y4CLUKUmSRtQp3Kerqq9W1dOALYFXAr8A7gu8a0y1SZKkeZh3uE+pqjVV9S9VdU9gd5pd9ZIkaULGcuGYKVW1Clg1znVKkqRuFrzlLkmSlhfDXZKknjHcJUnqGcNdkqSeMdwlSeoZw12SpJ4x3CVJ6hnDXZKknjHcJUnqGcNdkqSeMdwlSeoZw12SpJ4x3CVJ6hnDXZKknjHcJUnqGcNdkqSeMdwlSeoZw12SpJ4x3CVJ6pmJh3uSzZI8L8lxSX6V5Moklyb5ZpLnJpl4jZIkrSTrTLoA4CnA4cB5wEnAb4E7AE8C3g88JslTqqomV6IkSSvHcgj3XwCPBz5fVddPzUzyGuB7wJNpgv5TkylPkqSVZeK7vKvqa1X12enB3s4/H3hfe3fXJS9MkqQVauLhPodr2um1E61CkqQVZNmGe5J1gGe3d0+YZC2SJK0kyzbcgUOAewNfqKovDWuQ5IAkpyY59ZorLlnS4iRJWq6WZbgnOQh4OfBzYJ+Z2lXVkVW1Y1XteIv1N16q8iRJWtaWXbgnORD4N+CnwG5VdfGES5IkaUVZVuGe5KXAu4Ef0wT7+ZOtSJKklWfZhHuSVwGHAmfQBPuFk61IkqSVaVmEe5LX0XSgOw3Yo6rWTLgkSZJWrImPUJdkX+BNwHXAycBBSQabra6qo5e4NEmSVqSJhztwt3a6NvDSGdp8HTh6KYqRJGmlm/hu+ao6uKoyx23XSdcpSdJKMfFwlyRJ42W4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzhrskST1juEuS1DOGuyRJPWO4S5LUM4a7JEk9Y7hLktQzqapJ1zAWSf4AnD3pOlaA2wJrJl2EesXvlMbN79Ro7lpVtxu2oDfhrtEkObWqdpx0HeoPv1MaN79TC+dueUmSesZwlySpZwz3m58jJ13AzVGSSrJqYN7B7fxdJ1JUR7PU63dK4+Z3aoEM95uZqurtP5o2eKbfrkuyJsnXkjxz0vUthmE/GpbaXN+pJLdM8twkn09yXpI/J7k8yRlJ3pnkPgt5/iS7tu/DwQtZj5aPPv8/tVTWmXQB0iJ4Yzu9BbAdsCewW5L7V9X/nlhVN/Vu4OPAbyddyGJJsi3wGeCeNL2fv0zzem8J/DXwAuCgJHtW1X9Nqk6pbwx39U5VHTz9fpI9aELlpUneVVWrJ1HXoKpaQ49P90lyB+CrwJbAO4HXVNWVA21uD7wB2GTJC5R6zN3y6r2q+irwcyDAA+DGx4+T7J3ku0n+lGT11OOSrJfk/7S7j69ol38nyTOGPU+7+/l1SX7d7nr+TZK3JFl3hvYzHnNPco8kRyVZ3a7rwiQnJ3lhu3y/JFPnsT584HDEwQPrelCSTyY5P8nVSc5JckSSO81Q1/2TnNDuOr8syVeSPHj2d3mot9AE+8eq6mWDwQ5QVRdW1YE0ezCmnn/bJIckOTXJH9rXf3aSI5NsOVDr0cBJ7d03DLwPuw60fUaSk5L8MclVSX6W5LWzfD7PTHJ6kivb9//DSe6UZNW09356+7WSvCDJf7fflSvav1+Y5Cb/104dUkmyeZL3Jzm3PZS0X5KPt8sfNkNte7XLDxu2XHLLXTcXaaeD/ym/HHgE8FmakNgIIMnGwNeAHYDTgaNofgw/Cjg2yb2q6rV/WXkS4D+BJwC/ptnlfkvgOcDfdCo0+V/AJ4B1gROAjwEbA/cFXgkcDpxBc/jhDTSDNx09bRWrpq1rf+DfgT8D/wWcA9wdeB7wuCQ7VdVvp7XfGfhKW/ungV8B27fr/FqH13BrYJ/27htnawtQVX+edvdJNLvrTwK+DVwN3GtazTtW1blt28+0032BrzPttQOrp9XzAZrP4nc0r+sSYCfgzcAeSR5RVddOa/+PwNuBPwIfAi6l+Z58q/17mA8De9O8x++n+a49EXgv8BBgWL+PTYFTgD+1dV0PXNA+5mnA84FvDHncAe3UY9Marqq8eevFjeY/0xoy/29p/tO8nmZEJ4CD2/ZXADsMeczR7fJXDsy/FU3gXg9sP23+3m377wC3mjZ/U5qwL2DVwLqmath12rzb0oTH1cDDh9S15ZDXvGqwXbts23Y9vwK2GFi2O3AdcNy0eaHZw1HAEwbav2Tq/Z1e7yyfxUPbtr+bx+e4BbDukPmPbGs+fGD+ru1zHTzD+vZrl38auPUMn8FLps3bGrgG+ANw54H352PDvmfAM9r5pwO3mTZ/feDUdtnew76vwDHAOkPq/jFwFXDbgfl3a79/31qqf1veVt7N3fLqnXZ398FJ3prkkzRhHOCdVTU4RPGRVfX9gcdvBjwLOLWq3j59WVVdBbyqXd/e0xbt305f07aZan8xzdbhqPYFNqQJsK8PLqyq33VY1wtpOhW+pG7Y0p1az9dotuQfl2SDdvbONB0Qv1FVxw+s6900P1JGdcd22qXeqdrOrRtvyU/NPxH4Cc3eky5eAlwLPKduemjgzcBF3Hirem+avZqHVdU5056/gFfT/MAY9Jx2+uqq+tO0x1xB832BZs/DoKuBV9S0vQbTHE6z92bfgfkH0Hz/jhjyGAlwt7z66Q3ttGh2v54MfKCqPjKk7feGzHsAsDYw0+lVt2in95w27340W1PfHNJ+1ZwV32CndvrFDo+ZydRx8ocnecCQ5beneZ3bAqfRvAZodm/fSFVdl+SbwDYjPvdMh0HmfmBziOOZNFvc96XpbLf2tCZXd1jXeu061tB0qBzW7M/c+LPcoZ3e5LOsqrOTnANsNbBo6vNfNWT9X6f5QbDDkGWrq+rCGco/BjiEJsz/H0CSW9C8L3+kOQwkDWW4q3eqauj/4DM4f8i8zdrpA9rbTG4z7e+NgIur6poRn2MmG7fTc2drNKKp1/GPc7Sbeh0btdMLZmjX5XX8vp1uOWur4f4VeClwHvAlmvdiaot7P+CuHda1Cc0Pjdtxw4++ucz1PlzATcN96vO/yQ+Pqro2yRqaH1ODZnxPq+ryJB8BXpBkt6o6iaZPx+Y0e6GumumxkuGum7thW5ZTHaYOrdHPi78U2DTJLYYE/OYd6rmknW4B/KjD42aqCWCjqrqsQ/s7zLC8y+s4lWaLeMsk21XVmaM8KM2pcQfRHG/euaouH1g+9EyFWUy9pu9X1f1mbXmDqffqDjSHAQYNe39m/PyTrEPTl2LYZzDXno3DaToXPp+mg6Ed6TQSj7lLN/U9ml2sD+3wmNNp/j09ZMiyXTus55R2+pgR21/PjXdZD1vXqK/j9Hb68MEFSdZm+Gsbqj22/eH27uvmaj/tdLStad7HE4cE+5bt8kFTx8Bv8j60x79/AtwryaajVc9UH4ybvN4kdwXuPMNj1gKGnbr2sLa204csm1VV/ZCmh/4TkzyIpnPoN6rqZ13XpZsXw10a0B4D/SiwY5rz1m+yhyvJNknuNm3WB9vpW5Pcalq7TYHXMroP0WzhvXDYOc6D53nTdAYbFjbQdIK7Bjg0zUhxg+u6ZZLpwf9t4EzgYUmeMND8xYx+vH3Ka2k61D0zyTva0+MGa7htkncBT29nrW6nD2l/UEy1uw3NKX3D9jZe1E7vMkMd/0pzat9R7SmOgzVskmT6Vv2xNB3w/iHJnae1C/DPDP8xdVQ7/ef2OP/UY9ajOW4O8IEZ6pvL4W39n6I5xPC+ea5HNyPulpeGezHN+eBvAvZpO5NdANyJpvPVA2hOf/pN2/5jNOclPx74cZLjaTre7QX8NyMGY1WtSbI38EngpCRfBH5I04P+PjRBPv1HxVeBpyf5LE2nuGtptuy+UVU/T/IcmuD5SZITgF+0dd2FZov+D8A92ueuJM+lGc3vU0mmznO/L80W4wnAo0d7+6CqLkgzOuBngFcA+yaZPvzsPWn2aqxLM0QwVXV+ko/ThP0ZSU6kOZ79CJrTws6gOe9+ujNpjss/PcnV7foL+HBVnV1VRyW5P/Ai4NdJvtS22bR9Lx9G8+PsBW0Nv07yeuD/Aj9I8h/ccJ77psAP2s9i+ms9tv1B9NT2vf5MW8Oe7XP8Z1V9dNT3bsAngENpDtWsoTmlT5rdpM/F8+ZtXDdmOM99hrYHM8c52zQB9GKaLdpLaY4h/5YmUF8KbDak/euBs9q2q4G30oTXSOe5T1t2L5re0ufS9A6/gKbX9QED7W5Ps6V5Ac3u6Zuc700ziM7RNIPd/Bm4mOaY9hHA7kOe+/40QX55e/sKTc/7Od+zWd7H5wJfoOkkd3W73h8B7wL+ZqD9eu379iuaQD8HeA9NB8FVwz5jmh9bX20/p+uH1Qk8FvgccGFbw/k0h2DeAtxjyDr3odndfhXNj6CP0Py4+zFwyZD2a9H8gDgV+J/2dhpwILDWDN/XVSO+h4e27d8x6X9n3lbGLVWdz1SRpJulJBvS/JA6o6rmMyTvfJ93Fc0ehu2q6pdL9bxauTzmLkkDktyuPad8+rx1aM43vxVw3BLW8kCaTo5fMtg1KrfcJWlAkhfQ9Lf4Cs1hgU1ptpy3pTnuv3MNuRDOmGt4Ic1x9v1pTkN8cFUNG3RJugk71EnSTX2XZoS6h3HDYEC/oekL8LbFDvbWq2gGAToL2MdgVxduuUuS1DMec5ckqWcMd0mSesZwlySpZwx3SZJ6xnCXJKlnDHdJknrm/wMjtxMlv+EqOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40.99953183520599"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_accuracy(rnn_model, dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.445     0.426     0.435      3610\n",
      "           1      0.396     0.608     0.479      3310\n",
      "           2      0.000     0.000     0.000      1624\n",
      "\n",
      "    accuracy                          0.415      8544\n",
      "   macro avg      0.280     0.345     0.305      8544\n",
      "weighted avg      0.341     0.415     0.370      8544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_stats(rnn_model, dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations\n",
    "\n",
    "As usual this is a very nave model, just for demonstration.\n",
    "It lacks many tricks of the NLP trade, such was pre-trained embeddings,\n",
    "gated RNN units, deep or bi-directional models, dropout, etc.\n",
    "\n",
    "Don't expect SotA results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Further reading\n",
    "---\n",
    "\n",
    "## LSTM and GRU\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We talked about the vanishing gradients problem with RNN, however, we didn't mention how to deal with those.\n",
    "Luckyly, Gradient based methods invented long time ago...\n",
    "Today we will talk about **Long Short term Memory (LSTM)** and **Gated Recurrent Unit (GRU)**. <br>\n",
    "\n",
    "both are a type of recurrent cell that tries to preserve long term information. The idea of LSTM was presented back in 1997, but flourished in the age of deep learning.\n",
    "\n",
    "The cell have `memory` that's base on the `state` of basic RNN and on 3 main gates: \n",
    "\n",
    "- Input gate: decides when to read data into the cell.\n",
    "- Output gate: outputs the entries from the cell.\n",
    "- Forget gate: a mechanism to reset the content of the cell.\n",
    "These gates learn which information is relevant to forget or remember during the training process. The gates contain a non-linear activision function (sigmoid).\n",
    "\n",
    "\n",
    "<center><img src=\"resources/LSTM.PNG\" width=\"800\" /></center>\n",
    "\n",
    "\n",
    "\n",
    "you can read about the history of LSTM that originated in 1997 in [Wiki](https://en.wikipedia.org/wiki/Long_short-term_memory), however, i recomend to focus on the GRU since the math behind it designed for our perpouse and more modern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs introduce a considerable number of additional parameters to our recurrent\n",
    "networks. We now have 8 sets of weights to learn (i.e., the U and W for each of the 4\n",
    "gates within each unit), whereas with simple recurrent units we only had 2. Training\n",
    "these additional parameters imposes a much significantly higher training cost. **GRUs** ease this burden by dispensing with the\n",
    "use of a separate context vector, and by reducing the number of gates to 2  a reset gate, r and an update gate, z.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"resources/GRU.png\" width=\"600\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI-DIRECTIONAL RNN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This units solve to some degree the vanishing gradients. please note that non of them have resiliance to exploding gradients.<br>\n",
    "\n",
    "However, we didn't talk about the problem of future context.\n",
    "\n",
    "NLP has very complex tasks that we can't cover in this tutorial and deserve a course of itself.\n",
    "\n",
    "However, we can think of problems such as part-of speech (POS) taging or translation. and figure that for some tasks, future input is require in order to forcast the current prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"Where are you meeting him? here by the rocks, and his favorite food is roasted fox!\"\n",
    "\n",
    "\"   ?   ,      !\"\n",
    "<center><img src=\"resources/Fairuse_Gruffalo.jpg\" width=\"200\" /></center>\n",
    "\n",
    "roasted fox start with adjective and then noun, while in hebrew it's more common to have the noun first and then the adjective. how can we translate the sentence if the hidden state does not contain the future noun?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that we use Bi-directional RNN!\n",
    "<center><img src=\"resources/bi.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/pubs/glove.pdf glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by [Aviv A. Rosenberg](https://avivr.net).<br>\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "- http://d2l.ai\n",
    "- http://giphy.com \n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Sebastian Ruder, \"On word embeddings - Part 1\", 2016, https://ruder.io\n",
    "- http://karpathy.github.io\n",
    "- Stanford cs231n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
